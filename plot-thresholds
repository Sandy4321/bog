#!/usr/bin/python
import cPickle
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import argparse
import time

from operator import neg

from meta import load_opinions
from munge import best_connections, p_to_affinities, data_to_clusters
from munge import clipped_neg_exp, clipped_logistic, shuffle_array

from eval import load_clustering_json, load_ranking_json, calc_map


def calc_fbcubed(truth, clusters):
    r = 0.0
    p = 0.0
    n = 0.0
    for cluster in clusters.values():
        n += len(cluster)
        for d in cluster:
            tcluster = truth[d]
            correct = len(cluster & tcluster)
            pp = float(correct) / len(cluster)
            rr = float(correct) / len(tcluster)
            p += pp
            r += rr
    rp = n / p
    rr = n / r
    return 2.0 / (rp + rr)


def symmetricise(x):
    return (x + x.T) * 0.5


def norm2(x):
    return x - np.diagonal(x)


def _norm_texts(x, control_texts, control_models):
    return x - control_texts


def _norm_models(x, control_texts, control_models):
    return x - control_models


def _norm_both(x, control_texts, control_models):
    return 2 * x - control_texts - control_models


def _norm_none(x, control_texts, control_models):
    return x


LINE_WIDTH = 0.5

STRATEGIES = {
    'asymmetric': (neg,),
    'simple': (neg, symmetricise),
    'norm2': (neg, norm2),
    'norm2-symmetric': (neg, norm2, symmetricise),
    'best_connections': (neg, symmetricise, best_connections),
    'montecarlo': (symmetricise, clipped_neg_exp, p_to_affinities),
    'montecarlo-norm2': (norm2, symmetricise, clipped_neg_exp,
                         p_to_affinities),
    'montecarlo-asymmetric': (clipped_neg_exp, p_to_affinities),
    'montecarlo-sigmoid-asymmetric': (clipped_logistic, p_to_affinities),
    'montecarlo-sigmoid': (symmetricise, clipped_logistic,
                           p_to_affinities),
    'montecarlo-sigmoid-norm2': (symmetricise, norm2, clipped_logistic,
                                 p_to_affinities),
    'sigmoid': (symmetricise, clipped_logistic,),
    'sigmoid-asymmetric': (clipped_logistic,),
}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input',
                        help='input filename (pickle)')

    parser.add_argument('-g', '--ground-truth-dir',
                        help='cluster ground truth (json)')

    parser.add_argument('-n', '--normalise', default='texts',
                        choices=('texts', 'models', 'both', 'none'),
                        help='normailse in this dimension')

    parser.add_argument('--strategy', default='simple',
                        help='how to process the affinities (%s)' %
                        (', '.join(STRATEGIES.keys())))

    parser.add_argument('--n-clusters', action='store_true',
                        help='plot the number of clusters at each threshold')

    parser.add_argument('--n-links', action='store_true',
                        help='plot the number of links at each threshold')

    parser.add_argument('--delta', action='store_true',
                        help='changes in threshold')

    parser.add_argument('-m', '--show-map', action='store_true',
                        help='show mean average precision')

    parser.add_argument('--time', action='store_true',
                        help=('show no figure, print elapsed '
                              'time for first problem'))

    parser.add_argument('--shuffle', action='store_true',
                        help="randomly shuffle results (for baseline)")

    args = parser.parse_args()

    opinions = load_opinions(args.input)
    affinities = opinions['affinities']
    names = opinions['names']
    control_texts = opinions['control_texts']
    control_models = opinions['control_models']

    if args.time:
        start_time = time.time()

    if args.ground_truth_dir:
        true_clusters = {}
        for pid in affinities:
            fn = os.path.join(args.ground_truth_dir, pid, 'clustering.json')
            documents, clusters = load_clustering_json(fn)
            for cluster in clusters:
                for doc in cluster:
                    true_clusters[doc] = cluster

        if args.show_map:
            true_ranking = {}
            for pid in affinities:
                fn = os.path.join(args.ground_truth_dir, pid, 'ranking.json')
                true_ranking[pid] = load_ranking_json(fn)
        else:
            true_ranking = None
    elif args.show_map:
        print ("you need to supply --ground-truth-dir for --show-map")
        sys.exit(1)
    else:
        true_clusters = None
        true_ranking = None

    for tag, data in affinities.items():
        print tag
        norm = globals.get('_norm_' + args.normalise)
        data = norm(data, control_texts, control_models)

        functions = STRATEGIES[args.strategy]
        for func in functions:
            data = func(data)

        if args.shuffle:
            data = shuffle_array(data)

        thresholds = np.unique(data)

        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111)
        ax.set_title(tag)
        ax.plot(thresholds, 'r.', linewidth=LINE_WIDTH, label='thresholds')

        if true_ranking:
            map = calc_map(data, true_ranking, documents)
            print "MAP is %f" % map
            ax.set_title("%s - %.5e" % (tag, map))

        if args.delta:
            deltas = np.diff(thresholds) * 10
            ax.plot(deltas, 'c.', linewidth=LINE_WIDTH, label='delta')

        if args.n_clusters:
            n_clusters = []
            prev_n = -1e99
            prev_t = -1e99
            for t in thresholds:
                clusters = data_to_clusters(data, t)
                n = len(clusters)
                n_clusters.append(n / float(data.shape[0]))
                if n < prev_n:
                    print "prev     %d, threshold %s %s" % (prev_n, prev_t,
                                                            sorted(prev_clusters))
                    print "clusters %d, threshold %s %s" % (n, t,
                                                            sorted(clusters))
                prev_n = n
                prev_t = t
                prev_clusters = clusters

            ax.plot(n_clusters, 'g.', linewidth=LINE_WIDTH, label='n clusters')

        if args.n_links:
            n_links = []
            w = data.shape[0]
            scale = 1.0 / (w * w)
            prev_links = 1e99
            prev_t = -1e99
            for t in thresholds:
                links = np.sum(data > t)
                if links > prev_links:
                    print "prev links %d, threshold %s" % (prev_links, prev_t)
                    print "     links %d, threshold %s" % (links, t)
                prev_links = links
                prev_t = t
                n_links.append(links * scale)
            ax.plot(n_links, 'y.', linewidth=LINE_WIDTH, label='n links')

        if true_clusters is not None:
            fbcubed = []
            old_clusters = None
            score = None
            best_fbcubed = -1
            best_fbcubed_c = 0
            for t in thresholds:
                clusters = data_to_clusters(data, t, names[tag])
                if clusters != old_clusters:
                    score = calc_fbcubed(true_clusters, clusters)
                    old_clusters = clusters
                    if score > best_fbcubed:
                        best_fbcubed = score
                        best_fbcubed_c = len(clusters)
                fbcubed.append(score)

            ax.plot(fbcubed, 'b.', linewidth=LINE_WIDTH, label='fbcubed')
            print "FBCUBED best: %.3f at %d/%d" % (best_fbcubed,
                                                   best_fbcubed_c,
                                                   data.shape[0])


        plt.legend(loc='center left', numpoints=1, frameon=False,
                   borderpad=0)

        if args.time:
            elapsed = time.time() - start_time
            print "%s %s took %.2f seconds" % (tag, data.shape, elapsed)
            sys.exit()
        plt.show()

main()
