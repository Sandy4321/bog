#!/usr/bin/python
import cPickle
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import argparse
import time

from operator import neg

from meta import load_opinions
from munge import p_to_affinities, data_to_clusters
from munge import clipped_neg_exp, clipped_logistic, shuffle_array

from eval import load_clustering_json, load_ranking_json, calc_map


def load_all_opinions(filenames):
    opinions = load_opinions(filenames[0])
    affinities = opinions['affinities']
    names = opinions['names']
    control_texts = opinions['control_texts']
    control_models = opinions['control_models']

    if len(filenames) > 1:
        for fn in filenames:
            opinions = load_opinions(fn)
            names2 = opinions['names']
            control_texts2 = opinions['control_texts']
            control_models2 = opinions['control_models']
            if (names2 != names):
                raise ValueError("Multiple input files are not compatible")

            affinities2 = opinions['affinities']
            for k in affinities:
                affinities[k] += affinities2[k]
                control_texts[k] += control_texts2[k]
                control_models[k] += control_models2[k]

        for k in affinities:
            scale = 1.0 / len(filenames)
            affinities[k] *= scale
            control_texts[k] *= scale
            control_models[k] *= scale

    return affinities, names, control_texts, control_models


def calc_fbcubed(truth, clusters):
    r = 0.0
    p = 0.0
    n = 0.0
    for cluster in clusters.values():
        n += len(cluster)
        for d in cluster:
            tcluster = truth[d]
            correct = len(cluster & tcluster)
            pp = float(correct) / len(cluster)
            rr = float(correct) / len(tcluster)
            p += pp
            r += rr
    rp = n / p
    rr = n / r
    return 2.0 / (rp + rr)


def symmetricise(x):
    return (x + x.T) * 0.5


def norm_diagonal(x):
    return x - np.diagonal(x)


def _norm_texts(x, control_texts, control_models):
    return x - control_texts


def _norm_models(x, control_texts, control_models):
    return x - control_models


def _norm_both(x, control_texts, control_models):
    return 2 * x - control_texts - control_models


def _norm_none(x, control_texts, control_models):
    return x


LINE_WIDTH = 0.5

STRATEGIES = {
    'asymmetric': (neg,),
    'simple': (neg, symmetricise),
    'transpose': (np.transpose, neg),
    'diagonal': (neg, norm_diagonal),
    'diagonal-symmetric': (neg, norm_diagonal, symmetricise),
    'montecarlo': (symmetricise, clipped_neg_exp, p_to_affinities),
    'montecarlo-diagonal': (norm_diagonal, symmetricise, clipped_neg_exp,
                            p_to_affinities),
    'montecarlo-asymmetric': (clipped_neg_exp, p_to_affinities),
    'montecarlo-sigmoid': (symmetricise, clipped_logistic,
                           p_to_affinities),
    'sigmoid': (neg, symmetricise, clipped_logistic,),
    'transpose-sigmoid': (np.transpose, neg, clipped_logistic,),
    'sigmoid-asymmetric': (neg, clipped_logistic,),
}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', action='append',
                        help='input filename[s] (pickle)')

    parser.add_argument('-g', '--ground-truth-dir',
                        help='cluster ground truth (json)')

    parser.add_argument('-n', '--normalise', default='texts',
                        choices=('texts', 'models', 'both', 'none'),
                        help='normailse in this dimension')

    parser.add_argument('--strategy', default='simple',
                        choices=STRATEGIES.keys(),
                        help='how to process the affinities')

    parser.add_argument('--list-strategies', action='store_true',
                        help='print valid strategies and exit')

    parser.add_argument('--n-clusters', action='store_true',
                        help='plot the number of clusters at each threshold')

    parser.add_argument('--n-links', action='store_true',
                        help='plot the number of links at each threshold')

    parser.add_argument('--delta', action='store_true',
                        help='changes in threshold')

    parser.add_argument('-m', '--show-map', action='store_true',
                        help='show mean average precision')

    parser.add_argument('--time', action='store_true',
                        help=('show no figure, print elapsed '
                              'time for first problem'))

    parser.add_argument('--shuffle', action='store_true',
                        help="randomly shuffle results (for baseline)")

    parser.add_argument('--no-plot', action='store_true',
                        help="don't actually plot")

    parser.add_argument('--no-fbcubed', action='store_true',
                        help="don't do fbcubed even with ground truth")

    args = parser.parse_args()

    if args.list_strategies:
        for k in STRATEGIES:
            print k
        sys.exit()

    affinities, names, control_texts, control_models = load_all_opinions(args.input)

    fbcubed_best_sum = 0.0
    fbcubed_c_sum = 0.0
    map_sum = 0.0
    true_clusters = None
    true_ranking = None

    if args.time:
        start_time = time.time()

    if args.ground_truth_dir:
        if not args.no_fbcubed:
            true_clusters = {}
            for pid in affinities:
                fn = os.path.join(args.ground_truth_dir, pid, 'clustering.json')
                documents, clusters = load_clustering_json(fn)
                for cluster in clusters:
                    for doc in cluster:
                        true_clusters[doc] = cluster

        if args.show_map:
            true_ranking = {}
            for pid in affinities:
                fn = os.path.join(args.ground_truth_dir, pid, 'ranking.json')
                true_ranking[pid] = load_ranking_json(fn)

    elif args.show_map:
        print ("you need to supply --ground-truth-dir for --show-map")
        sys.exit(1)

    n_problems = len(affinities)

    for tag, data in affinities.items():
        print tag
        c_texts = control_texts[tag]
        c_models = control_models[tag]

        norm = globals()['_norm_' + args.normalise]
        data = norm(data, c_texts, c_models)

        functions = STRATEGIES[args.strategy]
        for func in functions:
            data = func(data)

        if args.shuffle:
            data = shuffle_array(data)

        thresholds = np.unique(data)

        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111)
        ax.set_title(tag)
        if args.no_plot:
            def plot(*args, **kwargs):
                pass
        else:
            plot = ax.plot

        plot(thresholds, 'r.', linewidth=LINE_WIDTH, label='thresholds')

        if true_ranking:
            documents = names[tag]
            links = []
            n = data.shape[0]
            for i in range(n):
                for j in range(i + 1, n):
                    links.append((data[i, j], (documents[i], documents[j])))

            map = calc_map(links, true_ranking[tag], documents)
            print "MAP is %f" % map
            ax.set_title("%s: MAP %.3g" % (tag, map))
            map_sum += map

        if args.delta:
            deltas = np.diff(thresholds) * 10
            plot(deltas, 'c.', linewidth=LINE_WIDTH, label='delta')

        if args.n_clusters:
            n_clusters = []
            prev_n = -1e99
            prev_t = -1e99
            prev_clusters = None
            for t in thresholds:
                clusters = data_to_clusters(data, t)
                n = len(clusters)
                n_clusters.append(n / float(data.shape[0]))
                if n < prev_n:
                    print ("prev     %d, threshold %s %s" %
                           (prev_n, prev_t, sorted(prev_clusters)))
                    print ("clusters %d, threshold %s %s" %
                           (n, t, sorted(clusters)))
                prev_n = n
                prev_t = t
                prev_clusters = clusters

            plot(n_clusters, 'g.', linewidth=LINE_WIDTH, label='n clusters')

        if args.n_links:
            n_links = []
            w = data.shape[0]
            scale = 1.0 / (w * w)
            prev_links = 1e99
            prev_t = -1e99
            for t in thresholds:
                links = np.sum(data > t)
                if links > prev_links:
                    print "prev links %d, threshold %s" % (prev_links, prev_t)
                    print "     links %d, threshold %s" % (links, t)
                prev_links = links
                prev_t = t
                n_links.append(links * scale)
            plot(n_links, 'y.', linewidth=LINE_WIDTH, label='n links')

        if true_clusters is not None:
            fbcubed = []
            old_clusters = None
            score = None
            best_fbcubed = -1
            best_fbcubed_c = 0
            for t in thresholds:
                clusters = data_to_clusters(data, t, names[tag])
                if clusters != old_clusters:
                    score = calc_fbcubed(true_clusters, clusters)
                    old_clusters = clusters
                    if score > best_fbcubed:
                        best_fbcubed = score
                        best_fbcubed_c = len(clusters)
                fbcubed.append(score)

            plot(fbcubed, 'b.', linewidth=LINE_WIDTH, label='fbcubed')
            print "FBCUBED best: %.3f at %d/%d" % (best_fbcubed,
                                                   best_fbcubed_c,
                                                   data.shape[0])
            fbcubed_best_sum += best_fbcubed
            fbcubed_c_sum += (best_fbcubed_c / float(data.shape[0]))

        if args.time:
            elapsed = time.time() - start_time
            print "%s %s took %.2f seconds" % (tag, data.shape, elapsed)
            sys.exit()

        if not args.no_plot:
            plt.legend(loc='center left', numpoints=1, frameon=False,
                       borderpad=0)
            plt.show()

    if true_ranking:
        print "average MAP is       %f" % (map_sum / n_problems)

    if true_clusters is not None:
        print ("average best fbcubed %f at %.1f%%" %
               (fbcubed_best_sum / n_problems,
                100.0 * fbcubed_c_sum / n_problems))

main()
