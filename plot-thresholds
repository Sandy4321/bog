#!/usr/bin/python
import cPickle
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import argparse
import time
from colour import GREY, C_NORMAL

from meta import load_opinions
from munge import p_to_affinities, data_to_clusters, array_to_link_pairs
from munge import clipped_neg_exp, clipped_logistic, shuffle_array
from munge import write_normalised_png

from eval import print_links
from eval import load_clustering_json, load_ranking_json, calc_map

from interpret import apply_interpret_options
from interpret import add_interpret_options

from language import TRAINING_CORPUS


def calc_fbcubed(truth, clusters):
    r = 0.0
    p = 0.0
    n = 0.0
    for cluster in clusters:
        n += len(cluster)
        for d in cluster:
            tcluster = truth[d]
            correct = len(cluster & tcluster)
            pp = float(correct) / len(cluster)
            rr = float(correct) / len(tcluster)
            p += pp
            r += rr
    rp = n / p
    rr = n / r
    return 2.0 / (rp + rr)


LINE_WIDTH = 0.5

def main():
    parser = argparse.ArgumentParser()
    add_interpret_options(parser, corpus_dir=TRAINING_CORPUS)
    parser.add_argument('-g', '--ground-truth-dir',
                        help='cluster ground truth (json)')

    parser.add_argument('--n-clusters', action='store_true',
                        help='plot the number of clusters at each threshold')

    parser.add_argument('--n-links', action='store_true',
                        help='plot the number of links at each threshold')

    parser.add_argument('--delta', action='store_true',
                        help='changes in threshold')

    parser.add_argument('-m', '--show-map', action='store_true',
                        help='show mean average precision')

    parser.add_argument('--print-links', action='store_true',
                        help='print links, highlighting truth')

    parser.add_argument('--time', action='store_true',
                        help=('show no figure, print elapsed '
                              'time for first problem'))

    parser.add_argument('--no-plot', action='store_true',
                        help="don't actually plot")

    parser.add_argument('--no-fbcubed', action='store_true',
                        help="don't do fbcubed even with ground truth")

    parser.add_argument('--png-save-dir',
                        help="save array images here")


    args = parser.parse_args()

    affinities, all_names = apply_interpret_options(args)

    fbcubed_best_sum = 0.0
    fbcubed_c_sum = 0.0
    map_sum = 0.0
    true_clusters = None
    true_ranking = None

    if args.time:
        start_time = time.time()

    if args.ground_truth_dir:
        if not args.no_fbcubed:
            true_clusters = {}
            for pid in affinities:
                true_clusters[pid] = {}
                fn = os.path.join(args.ground_truth_dir, pid, 'clustering.json')
                documents, clusters = load_clustering_json(fn)
                for cluster in clusters:
                    for doc in cluster:
                        true_clusters[pid][doc] = cluster

        if args.show_map or args.print_links:
            true_ranking = {}
            for pid in affinities:
                fn = os.path.join(args.ground_truth_dir, pid, 'ranking.json')
                true_ranking[pid] = load_ranking_json(fn)

    elif args.show_map or args.print_links:
        print ("you need to supply --ground-truth-dir for --show-map or --print_links")
        sys.exit(1)

    n_problems = len(affinities)

    for pid, data in affinities.items():
        print pid
        names = all_names[pid]

        if args.png_save_dir:
            fn = "%s/%s.png" % (args.png_save_dir, pid)
            write_normalised_png(data, fn)

        thresholds = np.unique(data)

        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111)
        ax.set_title(pid)
        if args.no_plot:
            def plot(*args, **kwargs):
                pass
        else:
            plot = ax.plot

        plot(thresholds, 'r.', linewidth=LINE_WIDTH, label='thresholds')

        if args.show_map:
            links = array_to_link_pairs(data, names)
            map = calc_map(links, true_ranking[pid], names)
            print "MAP is %f" % map
            ax.set_title("%s: MAP %.3g" % (pid, map))
            map_sum += map

        if args.print_links:
            links = array_to_link_pairs(data, names, True)
            print_links(links, true_ranking[pid], names)

        if args.delta:
            deltas = np.diff(thresholds) * 10
            plot(deltas, 'c.', linewidth=LINE_WIDTH, label='delta')

        if args.n_clusters:
            n_clusters = []
            prev_n = -1e99
            prev_t = -1e99
            prev_clusters = None
            for t in thresholds:
                clusters = data_to_clusters(data, t)
                n = len(clusters)
                n_clusters.append(n / float(data.shape[0]))
                if n < prev_n:
                    print ("prev     %d, threshold %s %s" %
                           (prev_n, prev_t, sorted(prev_clusters)))
                    print ("clusters %d, threshold %s %s" %
                           (n, t, sorted(clusters)))
                prev_n = n
                prev_t = t
                prev_clusters = clusters

            plot(n_clusters, 'g.', linewidth=LINE_WIDTH, label='n clusters')

        if args.n_links:
            n_links = []
            w = data.shape[0]
            scale = 1.0 / (w * w)
            prev_links = 1e99
            prev_t = -1e99
            for t in thresholds:
                links = np.sum(data > t)
                if links > prev_links:
                    print "prev links %d, threshold %s" % (prev_links, prev_t)
                    print "     links %d, threshold %s" % (links, t)
                prev_links = links
                prev_t = t
                n_links.append(links * scale)
            plot(n_links, 'y.', linewidth=LINE_WIDTH, label='n links')

        if true_clusters is not None:
            fbcubed = []
            old_clusters = None
            score = None
            best_fbcubed = -1
            best_fbcubed_c = 0
            for t in thresholds:
                clusters = data_to_clusters(data, t, names)
                if clusters != old_clusters:
                    score = calc_fbcubed(true_clusters[pid], clusters)
                    old_clusters = clusters
                    if score > best_fbcubed:
                        best_fbcubed = score
                        best_fbcubed_c = len(clusters)
                        best_fbcubed_t = t
                fbcubed.append(score)
            plot(fbcubed, 'b.', linewidth=LINE_WIDTH, label='fbcubed')
            print "FBCUBED best: %.3f at %d/%d (%.2f)" % (best_fbcubed,
                                                          best_fbcubed_c,
                                                          data.shape[0],
                                                          best_fbcubed_t)
            fbcubed_best_sum += best_fbcubed
            fbcubed_c_sum += (best_fbcubed_c / float(data.shape[0]))

        if args.time:
            elapsed = time.time() - start_time
            print "%s %s took %.2f seconds" % (pid, data.shape, elapsed)
            sys.exit()

        if not args.no_plot:
            plt.legend(loc='center left', numpoints=1, frameon=False,
                       borderpad=0)
            plt.show()

    if true_ranking:
        print "average MAP is       %f" % (map_sum / n_problems)

    if true_clusters is not None:
        print ("average best fbcubed %f at %.1f%%" %
               (fbcubed_best_sum / n_problems,
                100.0 * fbcubed_c_sum / n_problems))

main()
