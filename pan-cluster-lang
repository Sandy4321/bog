#!/usr/bin/python
# -*- coding: utf-8 -*-
import subprocess
import time
import os
import sys
HERE = os.path.dirname(__file__)
sys.path.append(HERE)
import argparse
import charmodel
import random
import json
import numpy as np
import errno

import language

from meta import write_results, makepath, save_opinions, load_opinions


def write_normalised_png(a, fn):
    #print a
    print fn
    from PIL import Image
    hi = np.amax(a)
    lo = np.amin(a)
    scale = 255.9 / (hi - lo)
    b = (a - lo) * scale
    c = np.array(b, dtype='uint8')
    print "low %f high %f scale %f" % (lo, hi,  scale)
    print "b min %f b max %f" % (np.amin(b), np.amax(b))
    print "c min %d b max %d" % (np.amin(c), np.amax(c))
    im = Image.fromarray(c)
    makepath(fn)
    im.save(fn)


def opine(net, texts, problems, ignore_start, symmetric=False,
          normalise=True):
    print "beginning opine"
    # Get the cross-entropy between all subnets against the texts of
    # each problem. Because texts are often in more than one problem,
    # we keep a cache (this is sort-of expensive).

    affinities = {}
    names = {}
    entropies_cache = {}

    for pid, records in problems.items():
        entropy_sum = 0.0
        entropy_matrix = []
        text_ids = [x[2] for x in records]
        entropy_means = []
        local_means = []
        for fn, ffn, tid in records:
            if tid in entropies_cache:
                entropies = entropies_cache[tid]
                print "using cached entropies for %s" % (tid,)
            else:
                text = texts[tid]
                # get entropies as a dictionary, indexed by fn.
                # (because we don't care about ones that aren't in the
                # problem, except to get an idea of average
                # cross-entropy).
                entropies = net.test(text, ignore_start, 0)
                entropies_cache[tid] = entropies

            local_entropies = [entropies[x] for x in text_ids]

            if normalise:
                #XXX should we use the "not in any problem" set?
                local_set = set(text_ids)
                control_entropies = [v for k, v in entropies.iteritems()
                                     if k not in local_set]
                control_entropy = sum(control_entropies)

                if len(control_entropies) > 1:
                    control_entropy /= len(control_entropies)

                local_mean = sum(local_entropies) / len(local_entropies)
                print ("found %d local entropies, µ %.3f, %d control µ %.3f "
                       "diff %.3f" % (len(local_entropies), local_mean,
                                      len(control_entropies), control_entropy,
                                      control_entropy - local_mean))
            else:
                control_entropy = 0.0

            entropy_matrix.append([x - control_entropy
                                   for x in local_entropies])

        score_matrix = np.array(entropy_matrix)
        if symmetric:
            score_matrix += score_matrix.T

        affinities[pid] = score_matrix
        names[pid] = [x[0] for x in records]

    return affinities, names


def entropy_to_p(raw_affinities):
    print raw_affinities
    x = np.maximum(raw_affinities, -40.0)
    x = np.minimum(x, 40.0)
    x *= 10
    sigmoid_affinities = 1.0 / (1.0 + np.exp(-x))
    open_affinities = np.exp(-x)
    return sigmoid_affinities, open_affinities


def p_to_affinities(input, samples=10000):
    """Treat the input array as a (somehow) scaled probability that two
    nodes are linked. The probability that they're in the same cluster
    has a transitive element.
    """
    totals = np.zeros_like(input)
    scale = 0.1
    min_scale = 1e-20
    max_scale = 1.0 / (1e-3 + np.min(input))
    runs = 100
    while runs > 0:
        scale = random.uniform(min_scale, max_scale)

        r = np.random.random(input.shape)
        p = input * scale

        links = r < p
        clusters = links_to_clusters(links)

        #print "found %2d clusters at %g" % (len(clusters), scale)

        if len(clusters) == 1:
            # everything is connected, so the scale is too high
            # make sure we don't go higher.
            max_scale = (scale + max_scale) * 0.5
            print "adjusting max to %.3g after %.3g" %  (max_scale, scale)
            continue

        if len(clusters) == input.shape[0]:
            #nothing is connected
            min_scale = (scale + min_scale) * 0.5
            print "adjusting MIN to %.3g after %.3g" %  (min_scale, scale)
            continue

        for v in clusters.values():
            #print v
            for x in v:
                for y in v:
                    if y != x:
                        totals[x, y] += 1.0

        runs -= 1
    totals *= 1.0 / (np.amax(totals) + 1e-10)
    return totals


def links_to_clusters(links):
    symlinks = links | links.T
    n = links.shape[0]
    cluster_map = range(n)
    for i in range(n):
        c = cluster_map[i]
        for j in range(n):
            if i != j and symlinks[i, j]:
                cluster_map[j] = c
    clusters = {}
    for i, c in enumerate(cluster_map):
        clusters.setdefault(c, []).append(i)
    return clusters


def affinities_to_clusters(affinities, threshold=None, n_min=None):
    if threshold is None and n_min is None:
        # lets try to find a threshold that will give us a middle
        # number of clusters.
        top = np.amax(affinities)
        bottom = np.amin(affinities)
        #threshold = (top + bottom) * 0.5
        threshold = np.percentile(affinities, 97)
        print ("trying threshold of %.3g (min %.3g, max %.3g)" %
               (threshold, bottom, top))

    if threshold is None:
        for threshold in sorted(set(affinities)):
            links = affinities > threshold
            clusters = links_to_clusters(links)
            if len(clusters) > n_min:
                break
    else:
        links = affinities > threshold
        clusters = links_to_clusters(links)

    return clusters.values()


def write_opinions(all_affinities, problems, output_dir):
    for pid, records in problems.items():
        raw_affinities = all_affinities[pid]
        sigmoid_affinities, open_affinities = entropy_to_p(raw_affinities)
        transitive_affinities = p_to_affinities(open_affinities)
        transitive_sigmoid_affinities = p_to_affinities(sigmoid_affinities)

        clusters = affinities_to_clusters(transitive_affinities)
        sigmoid_clusters = affinities_to_clusters(sigmoid_affinities)
        raw_clusters = affinities_to_clusters(raw_affinities)

        symmetric_raw = raw_affinities + raw_affinities.T
        fn = os.path.join(output_dir, '%s-%%s.png' % pid)
        print fn
        write_normalised_png(raw_affinities, fn % 'raw_affinities')
        write_normalised_png(symmetric_raw, fn % 'symmetric_raw')
        write_normalised_png(sigmoid_affinities, fn % 'sigmoid_affinities')
        write_normalised_png(open_affinities, fn % 'open_affinities')
        write_normalised_png(transitive_sigmoid_affinities,
                             fn % 'transitive_sigmoid_affinities')
        write_normalised_png(transitive_affinities,
                             fn % 'transitive_affinities')

        docnames = [x[0] for x in records]
        write_results(docnames, pid, transitive_affinities, clusters,
                      output_dir)
        #write_results(docnames, pid, sigmoid_affinities, sigmoid_clusters,
        #              output_dir, tag='sigmoid')
        #write_results(docnames, pid, raw_affinities, raw_clusters,
        #              output_dir, tag='raw')


def calc_ventropy_change(ventropies, prev_ventropies):
    ve_sum = 0.0
    ve_sum2 = 0.0
    ve_diff_sum = 0.0
    ve_diff_sum2 = 0.0
    for prev, e in zip(prev_ventropies, ventropies):
        diff = prev - e
        ve_diff_sum += diff
        ve_diff_sum2 += diff * diff
        ve_sum += e
        ve_sum2 += e * e
    ve_scale = 1.0 / len(ventropies)
    ve_mean = ve_sum * ve_scale
    ve_stddev = (ve_sum2 * ve_scale - ve_mean * ve_mean) ** 0.5
    ve_diff_mean = ve_diff_sum * ve_scale
    ve_diff_stddev = (ve_diff_sum2 * ve_scale -
                      ve_diff_mean * ve_diff_mean) ** 0.5

    return (ve_mean, ve_stddev, ve_diff_mean, ve_diff_stddev)


def train(net, texts, leakage, epochs, leakage_decay,
          learn_rate_decay, ignore_start, validation_text):

    prev_ventropies = net.test(validation_text, ignore_start, 1)

    for i in range(epochs):
        print ("starting epoch %d with learn-rate %s, "
               "leakage %s" % (i + 1, net.learn_rate, leakage))

        for name, text in texts.items():
            net.train(text, name, leakage=leakage,
                      ignore_start=ignore_start)

        ventropies = net.test(validation_text, ignore_start, 1)
        vstats = calc_ventropy_change(ventropies, prev_ventropies)
        prev_ventropies = ventropies
        print ("validation entropy %.3f±%.3f  diff % .3f±%.3f" % vstats)

        net.save()
        leakage *= leakage_decay
        net.learn_rate *= learn_rate_decay


def get_net_and_corpus(srcdir, controldir, control_n, lang,
                       reverse, validation_dir, net_kwargs):
    print srcdir
    texts, problems = language.load_corpus(srcdir, lang)

    if control_n and controldir:
        control_texts, _ = language.load_control_texts(controldir, lang)

        if len(control_texts) > control_n:
            # sort, to avoid hashing indeterminacy
            c_items = sorted(control_texts.items())
            random.shuffle(c_items)
            control_texts = dict(c_items[:control_n])

        texts.update(control_texts)

        print "using %d control texts; wanted %d" % (len(control_texts), control_n)

    # remap into net enumeration encoding
    alphabet = charmodel.Alphabet(''.join(texts.values()), ignore_case=False,
                                  threshold=1e-10)
    print >>sys.stderr, "alphabet is %s" % (alphabet.alphabet,)

    remapped = {}
    for k, v in texts.iteritems():
        t = alphabet.encode_text(v)
        if reverse:
            t = ''.join(reversed(t))
        remapped[k] = t

    if validation_dir:
        # validation text doesn't go in texts.
        v_texts, _ = language.load_control_texts(validation_dir, lang)
        v_text = alphabet.encode_text('\n'.join(v_texts.values()))
        if reverse:
            v_text = ''.join(reversed(v_text))

    textnames = sorted(texts.keys())

    metadata = json.dumps({
        'alphabet': alphabet.alphabet,
        'collapse_chars': alphabet.collapsed_chars,
        'version': 2,
        'class_names': textnames,
        'case_insensitive': False,
        'utf8': True,
        'collapse_space': False,
        'reverse': reverse,
    }, sort_keys=True)

    net = charmodel.Net(alphabet, textnames, metadata=metadata, **net_kwargs)
    return net, remapped, problems, v_text


def make_directory_name(basename, lang):
    # make up a good name
    here = os.path.dirname(__file__)
    git_hash = subprocess.check_output(['git', '-C', here,
                                        'rev-parse',
                                        '--short', 'HEAD']).strip()
    now = time.strftime('%Y-%m-%d+%H-%M-%S')
    mod = '-'
    if subprocess.call(['git', 'diff-files', '--quiet']):
        mod = '+'
    return 'results/%s-%s-%s%s-%s' % (basename, lang, git_hash, mod, now)


def main():
    parser = argparse.ArgumentParser()
    rnn_args = parser.add_argument_group('RNN arguments')

    parser.add_argument('lang', help="the language to look at")
    parser.add_argument('-i', '--input-dir', default=language.TRAINING_CORPUS,
                        help='find problems here')
    parser.add_argument('-o', '--output-dir',
                        help='write results here')

    parser.add_argument('--load',
                        help='load raw opinions from here')
    parser.add_argument('--save',
                        help='save raw opinions here')
    parser.add_argument('-S', '--auto-save', action='store_true',
                        help='save raw opinions in results dir')

    parser.add_argument('-n', '--basename', default='pan',
                        help="base filenames upon this")
    parser.add_argument('-v', '--verbose', action='store_true',
                        help="print more to stderr")
    parser.add_argument('-r', '--rng-seed', type=int, default=-1,
                        help="rng seed (-1 for auto)")

    parser.add_argument('-c', '--control-corpus',
                        default=language.CONTROL_CORPUS,
                        help="use this alternative control corpus")
    parser.add_argument('--validation-corpus',
                        default=language.VALIDATION_CORPUS,
                        help="use this text for validation")
    parser.add_argument('-C', '--control-n', default=0, type=int, #XXX up to n altogether?
                        help="how many controls to add")
    parser.add_argument('--reverse', action='store_true',
                        help="process all texts in reverse")

    rnn_args.add_argument('-H', '--hidden-size', type=int, default=199,
                          metavar='<nodes>', help="number of hidden nodes")
    rnn_args.add_argument('-e', '--sub-epochs', type=int, default=1,
                          help="how many cycles through the texts to do")
    rnn_args.add_argument('--batch-size', type=int, default=20,
                          metavar='<int>', help="mini-batch size")
    rnn_args.add_argument('--presynaptic-noise', type=float, default=0.1,
                          metavar='<float>',
                          help="Add this much presynaptic noise")
    rnn_args.add_argument('-l', '--learn-rate', type=float, default=1e-1,
                          help=charmodel.Net.learn_rate.__doc__)
    rnn_args.add_argument('-L', '--leakage', type=float, default=-2.0,
                          help=("how much training leaks into other classes "
                                "[0-1] or negative"))
    rnn_args.add_argument('--leakage-decay', type=float, default=0.9,
                          help="change in leakage per sub-epoch")
    rnn_args.add_argument('--learn-rate-decay', type=float, default=1,
                          help="change in learn-rate per sub-epoch")
    rnn_args.add_argument('-m', '--momentum', type=float, default=0.95,
                          metavar='<0-1>', help=charmodel.Net.momentum.__doc__)
    rnn_args.add_argument('--momentum-weight', type=float, default=0.5,
                          metavar='<0-1>',
                          help=charmodel.Net.momentum_weight.__doc__)
    rnn_args.add_argument('--log-file', default=None,
                          help="log to this file")
    rnn_args.add_argument('--enable-fp-exceptions', action='store_true',
                          help="crash on bad floating point errors")
    rnn_args.add_argument('--temporal-pgm-dump', action='store_true',
                          help=("save images showing changing state "
                                "of input/error vectors"))
    rnn_args.add_argument('--periodic-pgm-dump',
                          metavar='"({ih,ho,bi}{w,m,d,t})*"',
                          help=("Periodically dump images of weights;"
                                "string determines which"))
    rnn_args.add_argument('--periodic-pgm-period', type=int, default=10000,
                          help=("periodicity of periodic-pgm-dump"))
    rnn_args.add_argument('--learning-method', type=int, default=4,
                          help=("0: weighted, 2: simplified N., "
                                "3: classical, 4: adagrad"))
    rnn_args.add_argument('--activation', type=int, default=2,
                          help=("1: ReLU, 2: ReSQRT, 3: ReLOG, 4: "
                                "ReTANH, 5: clipped ReLU"))
    rnn_args.add_argument('-d', '--bptt-depth', type=int, default=50,
                          help="how far to backpropagate through time")
    rnn_args.add_argument('--ignore-start', type=int, default=0,
                          help="don't train on this many characters at start")
    rnn_args.add_argument('-f', '--filename',
                          help="save net here")
    rnn_args.add_argument('--init-method', type=int,
                          default=charmodel.INIT_FLAT,
                          help="0: zeros, 1: flat, 2: fan-in, 3: runs")

    args = parser.parse_args()

    if args.enable_fp_exceptions:
        charmodel.enable_fp_exceptions()

    if args.rng_seed != -1:
        random.seed(args.rng_seed)

    if args.output_dir is None:
        dir = make_directory_name(args.basename, args.lang)
        args.output_dir = dir

    if not args.load and args.auto_save:
        args.save = os.path.join(args.output_dir, 'affinities.pickle')

    try:
        os.makedirs(args.output_dir)
        print "made output directory %r" % args.output_dir
    except OSError, e:
        if e.errno == errno.EEXIST:
            print "output directory %r already exists" % args.output_dir
        else:
            raise

    net_kwargs = {}
    for k, v in vars(args).items():
        if k in ("bptt_depth",
                 "hidden_size",
                 "rng_seed",
                 "log_file",
                 "verbose",
                 "learn_rate",
                 "temporal_pgm_dump",
                 "periodic_pgm_dump",
                 "periodic_pgm_period",
                 "basename",
                 "activation",
                 "learning_method",
                 "batch_size",
                 "filename",
                 "presynaptic_noise",
                 "init_method",
            ):
            net_kwargs[k] = v

    net, texts, problems, v_text = get_net_and_corpus(args.input_dir,
                                                      args.control_corpus,
                                                      args.control_n,
                                                      args.lang,
                                                      args.reverse,
                                                      args.validation_corpus,
                                                      net_kwargs)

    if args.load is None:
        train(net, texts, args.leakage, args.sub_epochs,
              args.leakage_decay, args.learn_rate_decay,
              args.ignore_start,
              v_text)
        affinities, names = opine(net, texts, problems, args.ignore_start)
    else:
        affinities = load_opinions(args.load)

    if args.save is not None:
        save_opinions(affinities, names, args.save)

    write_opinions(affinities, problems, args.output_dir)

main()
